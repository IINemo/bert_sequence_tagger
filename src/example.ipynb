{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "logger = logging.getLogger('sequence_tagger_bert')\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "logger.handlers = []\n",
    "\n",
    "fhandler = logging.handlers.TimedRotatingFileHandler(filename='logs.txt', when='midnight')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla V100-DGXS-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda')\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "for i in range(n_gpu):\n",
    "    print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = '../workdir/cache'\n",
    "#BATCH_SIZE = 16\n",
    "BATCH_SIZE = 8\n",
    "PRED_BATCH_SIZE = 1000\n",
    "MAX_LEN = 128\n",
    "#MAX_N_EPOCHS = 4\n",
    "MAX_N_EPOCHS = 100\n",
    "REDUCE_ON_PLATEAU = False\n",
    "WEIGHT_DECAY = 0.01\n",
    "#LEARNING_RATE = 3e-5\n",
    "LEARNING_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-26 21:45:56,524 Reading data from ../workdir/conll2003/eng\n",
      "2019-09-26 21:45:56,524 Train: ../workdir/conll2003/eng/train.txt\n",
      "2019-09-26 21:45:56,525 Dev: ../workdir/conll2003/eng/dev.txt\n",
      "2019-09-26 21:45:56,525 Test: ../workdir/conll2003/eng/test.txt\n",
      "{\n",
      "    \"TRAIN\": {\n",
      "        \"dataset\": \"TRAIN\",\n",
      "        \"total_number_of_documents\": 14987,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 204567,\n",
      "            \"min\": 1,\n",
      "            \"max\": 113,\n",
      "            \"avg\": 13.649629679055181\n",
      "        }\n",
      "    },\n",
      "    \"TEST\": {\n",
      "        \"dataset\": \"TEST\",\n",
      "        \"total_number_of_documents\": 3684,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 46666,\n",
      "            \"min\": 1,\n",
      "            \"max\": 124,\n",
      "            \"avg\": 12.667209554831704\n",
      "        }\n",
      "    },\n",
      "    \"DEV\": {\n",
      "        \"dataset\": \"DEV\",\n",
      "        \"total_number_of_documents\": 3466,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 51578,\n",
      "            \"min\": 1,\n",
      "            \"max\": 109,\n",
      "            \"avg\": 14.881130986728216\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "\n",
    "data_folder = '../workdir/conll2003/eng'\n",
    "corpus = ColumnCorpus(data_folder, \n",
    "                      {0 : 'text', 3 : 'ner'},\n",
    "                      train_file='train.txt',\n",
    "                      test_file='test.txt',\n",
    "                      dev_file='dev.txt')\n",
    "\n",
    "print(corpus.obtain_statistics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.6.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/root/.pyenv/versions/3.6.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/root/.pyenv/versions/3.6.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/root/.pyenv/versions/3.6.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/root/.pyenv/versions/3.6.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/root/.pyenv/versions/3.6.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from bert_sequence_tagger import SequenceTaggerBert, BertForTokenClassificationCustom, create_optimizer\n",
    "from pytorch_transformers import BertTokenizer, BertForTokenClassification\n",
    "\n",
    "from bert_sequence_tagger.bert_utils import make_bert_tag_dict_from_flair_corpus\n",
    "\n",
    "\n",
    "bpe_tokenizer = BertTokenizer.from_pretrained('bert-base-cased', cache_dir=CACHE_DIR, do_lower_case=False)\n",
    "\n",
    "idx2tag, tag2idx = make_bert_tag_dict_from_flair_corpus(corpus)\n",
    "\n",
    "model = BertForTokenClassificationCustom.from_pretrained('bert-base-cased', cache_dir=CACHE_DIR, num_labels=len(tag2idx)).cuda()\n",
    "#model = BertForTokenClassification.from_pretrained('bert-base-cased', cache_dir=CACHE_DIR, num_labels=len(tag2idx)).cuda()\n",
    "\n",
    "seq_tagger = SequenceTaggerBert(bert_model=model, bpe_tokenizer=bpe_tokenizer, \n",
    "                                idx2tag=idx2tag, tag2idx=tag2idx, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-26 21:50:49,117 - sequence_tagger_bert - INFO - Train loss: 0.8786812842545654\n",
      "2019-09-26 21:50:49,117 Train loss: 0.8786812842545654\n",
      "2019-09-26 21:50:58,612 - sequence_tagger_bert - INFO - Validation loss: 0.16806642711162567\n",
      "2019-09-26 21:50:58,612 Validation loss: 0.16806642711162567\n",
      "2019-09-26 21:50:58,613 - sequence_tagger_bert - INFO - Validation F1-Score: (0.6880476393024244, 0.7446436115637636)\n",
      "2019-09-26 21:50:58,613 Validation F1-Score: (0.6880476393024244, 0.7446436115637636)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   1%|          | 1/100 [04:43<7:48:27, 283.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-26 21:55:33,984 - sequence_tagger_bert - INFO - Train loss: 0.11844969308136374\n",
      "2019-09-26 21:55:33,984 Train loss: 0.11844969308136374\n",
      "2019-09-26 21:55:43,250 - sequence_tagger_bert - INFO - Validation loss: 0.06370066478848457\n",
      "2019-09-26 21:55:43,250 Validation loss: 0.06370066478848457\n",
      "2019-09-26 21:55:43,252 - sequence_tagger_bert - INFO - Validation F1-Score: (0.8778575004171534, 0.9081668419207851)\n",
      "2019-09-26 21:55:43,252 Validation F1-Score: (0.8778575004171534, 0.9081668419207851)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   2%|▏         | 2/100 [09:28<7:44:04, 284.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-26 22:00:09,026 - sequence_tagger_bert - INFO - Train loss: 0.06332421275229641\n",
      "2019-09-26 22:00:09,026 Train loss: 0.06332421275229641\n",
      "2019-09-26 22:00:18,244 - sequence_tagger_bert - INFO - Validation loss: 0.04755143076181412\n",
      "2019-09-26 22:00:18,244 Validation loss: 0.04755143076181412\n",
      "2019-09-26 22:00:18,246 - sequence_tagger_bert - INFO - Validation F1-Score: (0.9159066186929964, 0.9388895355604702)\n",
      "2019-09-26 22:00:18,246 Validation F1-Score: (0.9159066186929964, 0.9388895355604702)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   3%|▎         | 3/100 [14:03<7:34:54, 281.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-26 22:04:41,957 - sequence_tagger_bert - INFO - Train loss: 0.043285651388034495\n",
      "2019-09-26 22:04:41,957 Train loss: 0.043285651388034495\n",
      "2019-09-26 22:04:51,073 - sequence_tagger_bert - INFO - Validation loss: 0.038951643742620945\n",
      "2019-09-26 22:04:51,073 Validation loss: 0.038951643742620945\n",
      "2019-09-26 22:04:51,074 - sequence_tagger_bert - INFO - Validation F1-Score: (0.9315344004022459, 0.9501452643811737)\n",
      "2019-09-26 22:04:51,074 Validation F1-Score: (0.9315344004022459, 0.9501452643811737)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   4%|▍         | 4/100 [18:36<7:26:06, 278.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-26 22:09:23,211 - sequence_tagger_bert - INFO - Train loss: 0.029549416832729023\n",
      "2019-09-26 22:09:23,211 Train loss: 0.029549416832729023\n",
      "2019-09-26 22:09:32,482 - sequence_tagger_bert - INFO - Validation loss: 0.03621016349643469\n",
      "2019-09-26 22:09:32,482 Validation loss: 0.03621016349643469\n",
      "2019-09-26 22:09:32,484 - sequence_tagger_bert - INFO - Validation F1-Score: (0.9390560818174196, 0.9563752541388324)\n",
      "2019-09-26 22:09:32,484 Validation F1-Score: (0.9390560818174196, 0.9563752541388324)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   5%|▌         | 5/100 [23:17<7:22:41, 279.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-26 22:14:04,638 - sequence_tagger_bert - INFO - Train loss: 0.021122847122169663\n",
      "2019-09-26 22:14:04,638 Train loss: 0.021122847122169663\n",
      "2019-09-26 22:14:13,775 - sequence_tagger_bert - INFO - Validation loss: 0.03441258333623409\n",
      "2019-09-26 22:14:13,775 Validation loss: 0.03441258333623409\n",
      "2019-09-26 22:14:13,777 - sequence_tagger_bert - INFO - Validation F1-Score: (0.9458641560188299, 0.9590861405758246)\n",
      "2019-09-26 22:14:13,777 Validation F1-Score: (0.9458641560188299, 0.9590861405758246)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   6%|▌         | 6/100 [27:59<7:18:50, 280.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-26 22:18:52,349 - sequence_tagger_bert - INFO - Train loss: 0.016115604575272863\n",
      "2019-09-26 22:18:52,349 Train loss: 0.016115604575272863\n",
      "2019-09-26 22:19:01,461 - sequence_tagger_bert - INFO - Validation loss: 0.03568050405010581\n",
      "2019-09-26 22:19:01,461 Validation loss: 0.03568050405010581\n",
      "2019-09-26 22:19:01,463 - sequence_tagger_bert - INFO - Validation F1-Score: (0.9454514937898624, 0.9601209161725381)\n",
      "2019-09-26 22:19:01,463 Validation F1-Score: (0.9454514937898624, 0.9601209161725381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   7%|▋         | 7/100 [32:46<7:17:40, 282.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-26 22:23:35,784 - sequence_tagger_bert - INFO - Train loss: 0.012142518383372842\n",
      "2019-09-26 22:23:35,784 Train loss: 0.012142518383372842\n",
      "2019-09-26 22:23:44,954 - sequence_tagger_bert - INFO - Validation loss: 0.04033001232892275\n",
      "2019-09-26 22:23:44,954 Validation loss: 0.04033001232892275\n",
      "2019-09-26 22:23:44,956 - sequence_tagger_bert - INFO - Validation F1-Score: (0.9447852760736196, 0.9585323238206174)\n",
      "2019-09-26 22:23:44,956 Validation F1-Score: (0.9447852760736196, 0.9585323238206174)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   8%|▊         | 8/100 [37:30<7:13:29, 282.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-26 22:28:16,775 - sequence_tagger_bert - INFO - Train loss: 0.009664351197577093\n",
      "2019-09-26 22:28:16,775 Train loss: 0.009664351197577093\n",
      "2019-09-26 22:28:25,943 - sequence_tagger_bert - INFO - Validation loss: 0.042247312143445015\n",
      "2019-09-26 22:28:25,943 Validation loss: 0.042247312143445015\n",
      "2019-09-26 22:28:25,945 - sequence_tagger_bert - INFO - Validation F1-Score: (0.9427275780987573, 0.9585846867749419)\n",
      "2019-09-26 22:28:25,945 Validation F1-Score: (0.9427275780987573, 0.9585846867749419)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   9%|▉         | 9/100 [42:11<7:07:59, 282.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-26 22:32:56,639 - sequence_tagger_bert - INFO - Train loss: 0.008162430261471522\n",
      "2019-09-26 22:32:56,639 Train loss: 0.008162430261471522\n",
      "2019-09-26 22:33:06,393 - sequence_tagger_bert - INFO - Validation loss: 0.038676043041050434\n",
      "2019-09-26 22:33:06,393 Validation loss: 0.038676043041050434\n",
      "2019-09-26 22:33:06,395 - sequence_tagger_bert - INFO - Validation F1-Score: (0.9482412060301508, 0.9627696590118302)\n",
      "2019-09-26 22:33:06,395 Validation F1-Score: (0.9482412060301508, 0.9627696590118302)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 10/100 [46:51<7:02:30, 281.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-26 22:37:39,139 - sequence_tagger_bert - INFO - Train loss: 0.006518807236741245\n",
      "2019-09-26 22:37:39,139 Train loss: 0.006518807236741245\n",
      "2019-09-26 22:37:48,234 - sequence_tagger_bert - INFO - Validation loss: 0.04137409431859851\n",
      "2019-09-26 22:37:48,234 Validation loss: 0.04137409431859851\n",
      "2019-09-26 22:37:48,236 - sequence_tagger_bert - INFO - Validation F1-Score: (0.9498069498069499, 0.9616793626795371)\n",
      "2019-09-26 22:37:48,236 Validation F1-Score: (0.9498069498069499, 0.9616793626795371)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  11%|█         | 11/100 [51:33<6:57:53, 281.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-26 22:42:14,409 - sequence_tagger_bert - INFO - Train loss: 0.004637632620961292\n",
      "2019-09-26 22:42:14,409 Train loss: 0.004637632620961292\n",
      "2019-09-26 22:42:23,606 - sequence_tagger_bert - INFO - Validation loss: 0.04289621999487281\n",
      "2019-09-26 22:42:23,606 Validation loss: 0.04289621999487281\n",
      "2019-09-26 22:42:23,608 - sequence_tagger_bert - INFO - Validation F1-Score: (0.9460525211846631, 0.9608925562205822)\n",
      "2019-09-26 22:42:23,608 Validation F1-Score: (0.9460525211846631, 0.9608925562205822)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  12%|█▏        | 12/100 [56:08<6:50:23, 279.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-26 22:46:54,742 - sequence_tagger_bert - INFO - Train loss: 0.00398789727350876\n",
      "2019-09-26 22:46:54,742 Train loss: 0.00398789727350876\n",
      "2019-09-26 22:47:03,953 - sequence_tagger_bert - INFO - Validation loss: 0.04656124021857977\n",
      "2019-09-26 22:47:03,953 Validation loss: 0.04656124021857977\n",
      "2019-09-26 22:47:03,955 - sequence_tagger_bert - INFO - Validation F1-Score: (0.9453446394089497, 0.9603724178062264)\n",
      "2019-09-26 22:47:03,955 Validation F1-Score: (0.9453446394089497, 0.9603724178062264)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  13%|█▎        | 13/100 [1:00:49<6:45:57, 279.98s/it]"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from bert_sequence_tagger.bert_utils import prepare_flair_corpus\n",
    "from bert_sequence_tagger.model_trainer_bert import ModelTrainerBert\n",
    "\n",
    "\n",
    "collate_fn = lambda inpt: tuple(zip(*inpt))\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = prepare_flair_corpus(corpus.train)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              sampler=train_sampler, \n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              collate_fn=collate_fn)\n",
    "\n",
    "val_dataset = prepare_flair_corpus(corpus.dev)\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            sampler=val_sampler, \n",
    "                            batch_size=PRED_BATCH_SIZE,\n",
    "                            collate_fn=collate_fn)\n",
    "\n",
    "optimizer = create_optimizer(model, full_finetuning=True, \n",
    "                             weight_decay=WEIGHT_DECAY, \n",
    "                             lr_body=LEARNING_RATE, \n",
    "                             t_total=(len(corpus.train) / BATCH_SIZE)*MAX_N_EPOCHS)\n",
    "\n",
    "trainer = ModelTrainerBert(model=seq_tagger, \n",
    "                           optimizer=optimizer, \n",
    "                           train_dataloader=train_dataloader, \n",
    "                           val_dataloader=val_dataloader,\n",
    "                           patience=2,\n",
    "                           reduce_on_plateau=False, \n",
    "                           number_of_steps=(len(corpus.train) / BATCH_SIZE)*MAX_N_EPOCHS)\n",
    "\n",
    "trainer.train(epochs=MAX_N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9143007822800387, 0.9306361914074436)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = prepare_flair_corpus(corpus.test)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                              sampler=test_sampler, \n",
    "                              batch_size=PRED_BATCH_SIZE,\n",
    "                              collate_fn=collate_fn)\n",
    "_, test_loss, test_f1 = seq_tagger.predict(test_dataloader, evaluate=True)\n",
    "test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.9143007822800387, 0.9306361914074436)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
